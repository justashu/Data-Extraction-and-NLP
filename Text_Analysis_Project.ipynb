{"cells":[{"cell_type":"markdown","metadata":{"id":"yzJVi5ds17Iu"},"source":["###  Importing all required libraries\n","---\n","\n","\n","\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1576,"status":"ok","timestamp":1659085802649,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"ukwZune82gJG","outputId":"d6794c4d-a8ff-43e9-852c-29c51f0dc91a"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["from bs4 import BeautifulSoup\n","import requests\n","import pandas as pd\n","import nltk\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","nltk.download('punkt')\n","import numpy as np\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","import string\n","import re"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":641,"status":"ok","timestamp":1659087203010,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"ngZESjxdQkEv","outputId":"233130a1-d79d-4342-8ebe-95d1ed922233"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘content/drive/Text_Analysis_Project’: No such file or directory\n"]}],"source":["!mkdir Text_Analysis_Project\n"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1659087658430,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"61HOIGBISW77","outputId":"6e403818-a0a5-4c26-9dcf-88fe986485b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content\n"]}],"source":["cd /content"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":615,"status":"ok","timestamp":1659087671546,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"UPRHbEjET_t_","outputId":"faf8e4a8-f870-49b4-8f78-b389b780adab"},"outputs":[{"name":"stdout","output_type":"stream","text":["100.txt\n","101.txt\n","102.txt\n","103.txt\n","104.txt\n","105.txt\n","106.txt\n","107.txt\n","108.txt\n","109.txt\n","10.txt\n","110.txt\n","111.txt\n","112.txt\n","113.txt\n","114.txt\n","115.txt\n","116.txt\n","117.txt\n","118.txt\n","119.txt\n","11.txt\n","120.txt\n","121.txt\n","122.txt\n","123.txt\n","124.txt\n","125.txt\n","126.txt\n","127.txt\n","128.txt\n","129.txt\n","12.txt\n","130.txt\n","131.txt\n","132.txt\n","133.txt\n","134.txt\n","135.txt\n","136.txt\n","137.txt\n","138.txt\n","139.txt\n","13.txt\n","140.txt\n","141.txt\n","142.txt\n","143.txt\n","144.txt\n","145.txt\n","146.txt\n","147.txt\n","148.txt\n","149.txt\n","14.txt\n","150.txt\n","15.txt\n","16.txt\n","17.txt\n","18.txt\n","19.txt\n","1.txt\n","20.txt\n","21.txt\n","22.txt\n","23.txt\n","24.txt\n","25.txt\n","26.txt\n","27.txt\n","28.txt\n","29.txt\n","2.txt\n","30.txt\n","31.txt\n","32.txt\n","33.txt\n","34.txt\n","35.txt\n","36.txt\n","37.txt\n","38.txt\n","39.txt\n","3.txt\n","40.txt\n","41.txt\n","42.txt\n","43.txt\n","44.txt\n","45.txt\n","46.txt\n","47.txt\n","48.txt\n","49.txt\n","4.txt\n","50.txt\n","51.txt\n","52.txt\n","53.txt\n","54.txt\n","55.txt\n","56.txt\n","57.txt\n","58.txt\n","59.txt\n","5.txt\n","60.txt\n","61.txt\n","62.txt\n","63.txt\n","64.txt\n","65.txt\n","66.txt\n","67.txt\n","68.txt\n","69.txt\n","6.txt\n","70.txt\n","71.txt\n","72.txt\n","73.txt\n","74.txt\n","75.txt\n","76.txt\n","77.txt\n","78.txt\n","79.txt\n","7.txt\n","80.txt\n","81.txt\n","82.txt\n","83.txt\n","84.txt\n","85.txt\n","86.txt\n","87.txt\n","88.txt\n","89.txt\n","8.txt\n","90.txt\n","91.txt\n","92.txt\n","93.txt\n","94.txt\n","95.txt\n","96.txt\n","97.txt\n","98.txt\n","99.txt\n","9.txt\n","content.zip\n","drive\n","sample_data\n","Text_Analysis_Project\n","updating: 100.txt (deflated 52%)\n","updating: 101.txt (deflated 41%)\n","updating: 102.txt (deflated 53%)\n","updating: 103.txt (deflated 49%)\n","updating: 104.txt (deflated 57%)\n","updating: 105.txt (deflated 60%)\n","updating: 106.txt (deflated 53%)\n","updating: 107.txt (deflated 52%)\n","updating: 108.txt (deflated 54%)\n","updating: 109.txt (deflated 53%)\n","updating: 10.txt (deflated 57%)\n","updating: 110.txt (deflated 55%)\n","updating: 111.txt (deflated 59%)\n","updating: 112.txt (deflated 51%)\n","updating: 113.txt (deflated 54%)\n","updating: 114.txt (deflated 57%)\n","updating: 115.txt (deflated 57%)\n","updating: 116.txt (deflated 54%)\n","updating: 117.txt (deflated 58%)\n","updating: 118.txt (deflated 57%)\n","updating: 119.txt (deflated 52%)\n","updating: 11.txt (deflated 58%)\n","updating: 120.txt (deflated 58%)\n","updating: 121.txt (deflated 56%)\n","updating: 122.txt (deflated 54%)\n","updating: 123.txt (deflated 55%)\n","updating: 124.txt (deflated 57%)\n","updating: 125.txt (deflated 56%)\n","updating: 126.txt (deflated 55%)\n","updating: 127.txt (deflated 58%)\n","updating: 128.txt (deflated 56%)\n","updating: 129.txt (deflated 52%)\n","updating: 12.txt (deflated 56%)\n","updating: 130.txt (deflated 60%)\n","updating: 131.txt (deflated 57%)\n","updating: 132.txt (deflated 56%)\n","updating: 133.txt (deflated 55%)\n","updating: 134.txt (deflated 53%)\n","updating: 135.txt (deflated 52%)\n","updating: 136.txt (deflated 54%)\n","updating: 137.txt (deflated 44%)\n","updating: 138.txt (deflated 47%)\n","updating: 139.txt (deflated 53%)\n","updating: 13.txt (deflated 60%)\n","updating: 140.txt (deflated 56%)\n","updating: 141.txt (deflated 54%)\n","updating: 142.txt (deflated 55%)\n","updating: 143.txt (deflated 53%)\n","updating: 144.txt (deflated 55%)\n","updating: 145.txt (deflated 58%)\n","updating: 146.txt (deflated 53%)\n","updating: 147.txt (deflated 57%)\n","updating: 148.txt (deflated 59%)\n","updating: 149.txt (deflated 55%)\n","updating: 14.txt (deflated 56%)\n","updating: 150.txt (deflated 57%)\n","updating: 15.txt (deflated 58%)\n","updating: 16.txt (deflated 54%)\n","updating: 17.txt (deflated 57%)\n","updating: 18.txt (deflated 57%)\n","updating: 19.txt (deflated 57%)\n","updating: 1.txt (deflated 59%)\n","updating: 20.txt (deflated 53%)\n","updating: 21.txt (deflated 54%)\n","updating: 22.txt (deflated 55%)\n","updating: 23.txt (deflated 57%)\n","updating: 24.txt (deflated 54%)\n","updating: 25.txt (deflated 56%)\n","updating: 26.txt (deflated 57%)\n","updating: 27.txt (deflated 59%)\n","updating: 28.txt (deflated 59%)\n","updating: 29.txt (deflated 58%)\n","updating: 2.txt (deflated 63%)\n","updating: 30.txt (deflated 52%)\n","updating: 31.txt (deflated 56%)\n","updating: 32.txt (deflated 59%)\n","updating: 33.txt (deflated 59%)\n","updating: 34.txt (deflated 51%)\n","updating: 35.txt (deflated 56%)\n","updating: 36.txt (deflated 52%)\n","updating: 37.txt (deflated 58%)\n","updating: 38.txt (deflated 57%)\n","updating: 39.txt (deflated 59%)\n","updating: 3.txt (deflated 60%)\n","updating: 40.txt (deflated 57%)\n","updating: 41.txt (deflated 56%)\n","updating: 42.txt (deflated 55%)\n","updating: 43.txt (deflated 57%)\n","updating: 44.txt (deflated 58%)\n","updating: 45.txt (deflated 56%)\n","updating: 46.txt (deflated 58%)\n","updating: 47.txt (deflated 60%)\n","updating: 48.txt (deflated 59%)\n","updating: 49.txt (deflated 55%)\n","updating: 4.txt (deflated 61%)\n","updating: 50.txt (deflated 59%)\n","updating: 51.txt (deflated 56%)\n","updating: 52.txt (deflated 58%)\n","updating: 53.txt (deflated 58%)\n","updating: 54.txt (deflated 58%)\n","updating: 55.txt (deflated 57%)\n","updating: 56.txt (deflated 54%)\n","updating: 57.txt (deflated 53%)\n","updating: 58.txt (deflated 57%)\n","updating: 59.txt (deflated 54%)\n","updating: 5.txt (deflated 60%)\n","updating: 60.txt (deflated 64%)\n","updating: 61.txt (deflated 55%)\n","updating: 62.txt (deflated 53%)\n","updating: 63.txt (deflated 54%)\n","updating: 64.txt (deflated 51%)\n","updating: 65.txt (deflated 57%)\n","updating: 66.txt (deflated 48%)\n","updating: 67.txt (deflated 57%)\n","updating: 68.txt (deflated 44%)\n","updating: 69.txt (deflated 57%)\n","updating: 6.txt (deflated 59%)\n","updating: 70.txt (deflated 53%)\n","updating: 71.txt (deflated 57%)\n","updating: 72.txt (deflated 58%)\n","updating: 73.txt (deflated 57%)\n","updating: 74.txt (deflated 53%)\n","updating: 75.txt (deflated 57%)\n","updating: 76.txt (deflated 59%)\n","updating: 77.txt (deflated 55%)\n","updating: 78.txt (deflated 57%)\n","updating: 79.txt (deflated 54%)\n","updating: 7.txt (deflated 57%)\n","updating: 80.txt (deflated 59%)\n","updating: 81.txt (deflated 59%)\n","updating: 82.txt (deflated 58%)\n","updating: 83.txt (deflated 54%)\n","updating: 84.txt (deflated 57%)\n","updating: 85.txt (deflated 58%)\n","updating: 86.txt (deflated 58%)\n","updating: 87.txt (deflated 57%)\n","updating: 88.txt (deflated 62%)\n","updating: 89.txt (deflated 58%)\n","updating: 8.txt (deflated 57%)\n","updating: 90.txt (deflated 59%)\n","updating: 91.txt (deflated 40%)\n","updating: 92.txt (deflated 54%)\n","updating: 93.txt (deflated 60%)\n","updating: 94.txt (deflated 56%)\n","updating: 95.txt (deflated 55%)\n","updating: 96.txt (deflated 59%)\n","updating: 97.txt (deflated 55%)\n","updating: 98.txt (deflated 57%)\n","updating: 99.txt (deflated 55%)\n","updating: 9.txt (deflated 57%)\n"]}],"source":["! ls -1\n","! zip content.zip *.txt"]},{"cell_type":"markdown","metadata":{"id":"kYG9OFMQ2k86"},"source":["**Taking all the required websites and generating the files of content from each website**"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":43894,"status":"ok","timestamp":1659085980115,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"ivat-XGQsSn_"},"outputs":[],"source":["websites = pd.read_csv('/content/drive/MyDrive/Data/Websites.csv')\n","output = pd.read_csv('/content/drive/MyDrive/Data/Output.csv')\n","\n","urls = websites['URL']  # all websites\n","content_list_of_all_websites = []  # list of content of all websites\n","personal_pronoun_count = []\n","\n","headers = {\n","    \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0\"\n","}\n","\n","def getContent(urls):    # will create a list of content from all the websites\n","    for url in urls :\n","        html_text = requests.get(url, headers=headers)\n","        soup = BeautifulSoup(html_text.content, 'html.parser')\n","        contents = soup.find_all('div', class_ = 'td-post-content')\n","        title = soup.find('header', class_ = 'td-post-title')\n","        title = title.find('h1').get_text()\n","        paragraph_clean = title\n","        personal_pronouns = ['I','we','my','ours','us']\n","        for data in contents:\n","\n","            para = data.text\n","            for pronoun in personal_pronouns:\n","                pp_count = len(re.findall(pronoun, para))\n","            personal_pronoun_count.append(pp_count)\n","            paragraph_clean += para.replace('\\xa0','') # removes unwanted text with ''\n","\n","        try:\n","             paragraph_clean = paragraph_clean.replace(soup.find('pre').text, '')\n","        except :\n","            pass\n","        content_list_of_all_websites.append(paragraph_clean)\n","\n","def createFiles():\n","    for i in range(len(content_list_of_all_websites)):\n","        file = open(str(i + 1) + '.txt', 'w')\n","        file.write(content_list_of_all_websites[i])\n","        file.close()\n","\n","\n","\n","getContent(urls)  # initilaised the above function\n","\n","createFiles()     # # creating text files named as respective url_id and saving the content in them\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":644,"status":"ok","timestamp":1659086985091,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"p8YS7BlASiEC"},"outputs":[],"source":["createFiles()"]},{"cell_type":"markdown","metadata":{"id":"7rxDOv4180Do"},"source":["# creating stop words from given folder in drive and removing the stopwords from content\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3069,"status":"ok","timestamp":1659083464260,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"BIt7LEbKC1VY"},"outputs":[],"source":["\n","allstopwords = []\n","\n","def stopWords():\n","    files = ['/content/drive/MyDrive/Data/StopWords/StopWords_Auditor.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Currencies.txt','/content/drive/MyDrive/Data/StopWords/StopWords_DatesandNumbers.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Generic.txt','/content/drive/MyDrive/Data/StopWords/StopWords_GenericLong.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Geographic.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Names.txt']\n","    stopwords = []\n","    for file in files:\n","        with open(file, 'rb') as f:\n","            a = f.read()\n","            stopwords.append(a.decode('latin-1').lower())\n","    for i in range(len(stopwords)):\n","        allstopwords.extend(stopwords[i].split())\n","\n","\n","stopWords()\n","allstopwords = set(allstopwords)\n","\n","# tokenization of the content using the stop words created above and creating the list of remaining words(after removing stop words)\n","\n","Content_withoutStopWords = []  # this contains the list of all the contents after removing the stop words\n","tokenized_content = []  # list of all the tokenized content\n","\n","def remove_Stop_Words():\n","    for content in content_list_of_all_websites:\n","        tokenizedContent = word_tokenize(content.lower())\n","        tokenized_content.append(tokenizedContent)\n","        content_without_stop_words_current = []\n","        for word in tokenizedContent:\n","            if word not in allstopwords:\n","                content_without_stop_words_current.append(word)\n","        Content_withoutStopWords.append(content_without_stop_words_current)\n","\n","remove_Stop_Words()\n","\n","#Total Words after cleaning in each content\n","\n","total_words_list_after_cleaning = []\n","\n","for word_list in Content_withoutStopWords:\n","    total_words_list_after_cleaning.append(len(word_list))\n","total_words_list_after_cleaning = np.array(total_words_list_after_cleaning)\n","\n","# print((Content_withoutStopWords))\n","# print(total_words_list_after_cleaning)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Muez8nUr9hc3"},"source":["**Creating a dictionary of Positive and Negative words**\n","\n","Calculating the following scores:\n","\n","\n","1.   Positive and Negative Score\n","2.   Polarity Score\n","3.   Subjectivity Score\n","\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1659083464261,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"aW9oX6yTC1aB"},"outputs":[],"source":["\n","positive_words = []\n","negative_words = []\n","\n","def pos_neg_words():\n","    files = ['/content/drive/MyDrive/Data/MasterDictionary/negative-words.txt','/content/drive/MyDrive/Data/MasterDictionary/positive-words.txt']\n","\n","    for file in files:\n","        if file == '/content/drive/MyDrive/Data/MasterDictionary/negative-words.txt':\n","            with open(file, 'rb') as f:\n","                    a = f.read()\n","                    negative_words.extend((a.decode('latin-1').lower()).split())\n","        else:\n","            with open(file, 'rb') as f:\n","                    a = f.read()\n","                    positive_words.extend((a.decode('latin-1').lower()).split())\n","\n","pos_neg_words()\n","positive_words = set(positive_words)\n","negative_words = set(negative_words)\n","\n","# print(positive_words)\n","# print(negative_words)\n","\n","# function to create dictionary of positive and negative words from the content collected from the websites\n","\n","positive_words_list_from_content = []\n","negative_words_list_from_content = []\n","\n","# function to create dictionary of positive and negative words from the content collected from the websites\n","\n","positive_words_list_from_content = []\n","negative_words_list_from_content = []\n","\n","def collect_Words_from_content():\n","    for words_of_each_content in Content_withoutStopWords:\n","        pos_words = []\n","        neg_words = []\n","        for word in words_of_each_content:\n","            if word in positive_words:\n","                pos_words.append(word)\n","            elif word in negative_words:\n","                neg_words.append(word)\n","        positive_words_list_from_content.append(set(pos_words))\n","        negative_words_list_from_content.append(set(neg_words))\n","\n","collect_Words_from_content()\n","# print(positive_words_list_from_content)\n","# print(negative_words_list_from_content)\n","\n","## Extracting Derived variables\n","\n","# Positive And Negative score\n","\n","PositiveScore = []\n","NegativeScore = []\n","def calculate_score():\n","    for p in positive_words_list_from_content:\n","        PositiveScore.append(len(p))\n","    for n in negative_words_list_from_content:\n","        NegativeScore.append(len(n))\n","\n","calculate_score()\n","PositiveScore = np.array(PositiveScore)\n","NegativeScore = np.array(NegativeScore)\n","# print(PositiveScore)\n","# print(NegativeScore)\n","\n","## Polarity score\n","Polarity_score = np.divide(np.subtract(PositiveScore,NegativeScore),(np.add(PositiveScore,NegativeScore)+0.000001))\n","\n","# print(Polarity_score)\n","\n","# Subjectivity Score\n","\n","Subjectivity_score = np.divide(np.add(PositiveScore,NegativeScore),(total_words_list_after_cleaning+0.000001))\n","\n","# print(Subjectivity_score)\n","# print(total_words_list_after_cleaning)"]},{"cell_type":"markdown","metadata":{"id":"kOk5iTjy-gau"},"source":["**Analysis of Readability (using nltk model stopwords) using the Gunning Fox index formula**\n","\n","Calculating the following in each content:\n","\n","\n","1.   Total number words \n","2.   Average Sentence Length\n","3.   Average Word Length\n","2.   Percentage Complex Words(>2 syllables)\n","1.   Syllables Per Word\n","2.   Personal Pronoun Count(calculated in 2 tab of this colab)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1302,"status":"ok","timestamp":1659083465560,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"nAwear0aC1cX","outputId":"6d8ace39-e1f6-4f95-bb2a-930c7367265b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[7.40100251 6.726      7.39700375 7.31046614 7.45692884 7.49122807\n"," 6.72099853 6.88302752 7.20332481 6.92060491 6.71767497 6.72630458\n"," 6.20528211 6.55366269 6.66320755 6.84010152 7.08145766 6.14109347\n"," 6.63995726 6.12190083 6.38148984 6.84776903 6.86272727 6.96124031\n"," 6.97201767 6.93039773 7.04143337 6.83733333 6.73969849 6.46312684\n"," 6.67594937 6.85826772 6.53225806 7.07878788 6.59067358 6.39962121\n"," 7.14359862 6.36835279 7.17412935 6.31419285 6.63627354 6.62078652\n"," 6.72019465 6.52       6.16626506 6.49551752 6.33817595 7.30681818\n"," 6.24062096 6.65681818 6.55475207 7.25901639 6.32876712 6.8381295\n"," 7.46171694 6.97368421 6.70588235 6.75672215 6.62376238 6.57677165\n"," 6.79283887 6.19313305 6.18303571 6.95431472 7.16315789 7.3655914\n"," 6.15023474 6.39449541 7.46676096 6.67880795 6.60568384 7.18227092\n"," 6.38375973 6.29221557 6.44827586 6.31532847 6.30991736 6.58811262\n"," 6.3243762  5.98109966 6.16256831 6.78091873 6.3700361  6.48081535\n"," 6.95794393 7.10699001 6.68944099 5.95991189 6.7447479  7.11122881\n"," 6.41489362 6.12908778 6.72150538 6.49471662 6.4910859  6.90976059\n"," 6.57394366 6.54660348 6.63036304 6.53966346 6.07619048 6.34087237\n"," 6.38265306 7.02137405 6.35393258 7.08560311 5.99159664 6.85945073\n"," 5.67159763 6.534375   7.01056338 6.95114007 6.51394422 6.37471783\n"," 6.43137255 5.44099379 6.84330986 6.09954058 7.10194175 6.60447761\n"," 5.98296199 6.26516634 6.69484937 6.78353909 6.86880466 6.61330561\n"," 6.94325482 6.6754717  6.62870159 6.50861287 6.81160799 6.75655431\n"," 7.51980983 7.5        5.85185185 7.03645833 6.15827338 6.91240876\n"," 7.13486005 6.36510501 6.38235294 6.73814042 6.79428571 6.69591528\n"," 7.03501946 7.17222222 6.60518135 6.71037464 7.59198113 6.85229202]\n","[ 874  987 2394 2426 2394 2503 1380  891 1698 2178 1658 1877 1562 1135\n"," 2123  771 1938  981 1860  849  829  761 2264  532 1435 1494 1886 2278\n"," 1991  671  814 1518 1320  732 1501 1018 2470 1464 2178 1754 1994 1386\n","  824 1704  727 2448 2339 1554 1449 1755 1928  668 1781 1146  972  390\n"," 1174 1505  793 2005  745  406 1228  847  401  201  776  198 1612  583\n"," 1105 2097 1668 1527  794 1271  469 1920  957  993 1287 1724 1028 1602\n","  880 1472 1878 3850 1865 1978  175 1017 1821 2005 1162 2242 1101 1203\n"," 1213 1630  185 1133  742 1371  971  534  629 1228  552 1179 1160 1225\n"," 1480 1654 2160  733 2230 1134  451 2541 1300 1881 2002 2424 2124  928\n"," 1927 1002  857 2206 2102 1630 1424  511  321  406  262  278  849 1143\n","  985 2120  697 1327 1021 1091 1862 1382  957 1171]\n"]}],"source":[" \n","PUNCT_TO_REMOVE = list(string.punctuation)\n","stpwrds = list(stopwords.words('english'))\n","PUNCT_TO_REMOVE.extend(stpwrds)\n","stop_words_from_nlkt = set(PUNCT_TO_REMOVE)\n","\n","# print(PUNCT_TO_REMOVE)\n","\n","Content_withoutStopWords_nltk = []  # this contains the list of all the contents after removing the stop words\n","total_character_count_after_cleaning = []\n","def remove_Stop_Words_nltk():\n","    for words in tokenized_content:\n","        content_without_stop_words_current = []\n","        count = 0\n","        for word in words:\n","            if word not in PUNCT_TO_REMOVE:\n","                content_without_stop_words_current.append(word)\n","                count += len(word)\n","        Content_withoutStopWords_nltk.append(content_without_stop_words_current)\n","        total_character_count_after_cleaning.append(count)\n","remove_Stop_Words_nltk()\n","\n","# print(Content_withoutStopWords_nltk)\n","\n","#Total Words after cleaning in each content using stopwords from nltk library\n","\n","total_words_list_after_cleaning_nltk = []\n","\n","for word_list in Content_withoutStopWords_nltk:\n","    total_words_list_after_cleaning_nltk.append(len(word_list))\n","\n","total_words_list_after_cleaning_nltk = np.array(total_words_list_after_cleaning_nltk)\n","total_character_count_after_cleaning = np.array(total_character_count_after_cleaning)\n","\n","# print(total_words_list_after_cleaning_nltk)\n","\n","\n","# Average Sentence Length\n","\n","total_no_of_sentences = []\n","for lst in tokenized_content:\n","      total_no_of_sentences.append(lst.count('.')+lst.count('?')+lst.count('!'))\n","\n","total_no_of_sentences = np.array(total_no_of_sentences)\n","\n","average_senetence_length = np.divide(total_words_list_after_cleaning_nltk,total_no_of_sentences)\n","\n","# Average Word Length\n","\n","average_word_length = np.divide(total_character_count_after_cleaning,total_words_list_after_cleaning_nltk)\n","\n","# print(total_no_of_sentences)\n","print(average_word_length)\n","\n","# Percentage of Complex words\n","no_of_complex_words = []\n","\n","# coount syllable\n","syllable_count = []\n","def syllables(word):\n","    count = 0\n","    vowels = 'aeiou'\n","    exception_end_Tup = ('a','e','i','o','u','ed','es','le','les')\n","\n","    if word[0] in vowels:\n","        count +=1\n","    for index in range(1,len(word)):\n","        if word[index] in vowels and word[index-1] not in vowels:\n","            count +=1\n","    if word.endswith(exception_end_Tup):\n","        count -= 1\n","    if count == 0:\n","        count += 1\n","\n","    return count\n","\n","for words in Content_withoutStopWords_nltk:\n","    i=0\n","    s_count = 0\n","    for word in words:\n","       syll_count = syllables(word)\n","       s_count += syll_count\n","       if syll_count >2:\n","           i+=1\n","    no_of_complex_words.append(i)\n","    syllable_count.append(s_count)\n","# print(Content_withoutStopWords_nltk)\n","# print(no_of_complex_words)\n","syllable_count = np.array(syllable_count)\n","print(syllable_count)\n","no_of_complex_words = np.array(no_of_complex_words)\n","percentage_complex_words = np.divide(no_of_complex_words,total_words_list_after_cleaning_nltk)*100\n","\n","#syllable count per word\n","\n","syllable_count_per_word = np.divide(syllable_count,total_words_list_after_cleaning_nltk)\n","\n","# print(percentage_complex_words)\n","\n","# Fog Index\n","\n","fog_index = 0.4*(np.add(percentage_complex_words,average_senetence_length))\n","\n","# print(fog_index)\n","\n","# Personal Pronouns\n","\n","# print(personal_pronoun_count)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Y-HG0_nyAMpg"},"source":["## Saving all the calculated terms to the CSV file"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1659083465560,"user":{"displayName":"Ashutosh Yadav","userId":"01431085878299642080"},"user_tz":-330},"id":"ZutX-PZhC1eb"},"outputs":[],"source":["output = pd.read_csv('/content/drive/MyDrive/Data/Output.csv')\n","# print(output['URL_ID'])\n","# print(output.columns)\n","\n","output['POSITIVE SCORE'] = PositiveScore\n","output['NEGATIVE SCORE'] = NegativeScore\n","output['POLARITY SCORE'] = Polarity_score\n","output['SUBJECTIVITY SCORE'] = Subjectivity_score\n","output['AVG SENTENCE LENGTH'] = average_senetence_length\n","output['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words\n","output['FOG INDEX'] = fog_index\n","output['AVG NUMBER OF WORDS PER SENTENCE'] = average_senetence_length ##???\n","output['COMPLEX WORD COUNT'] = no_of_complex_words\n","output['WORD COUNT'] = total_words_list_after_cleaning_nltk\n","output['SYLLABLE PER WORD'] = syllable_count_per_word\n","output['PERSONAL PRONOUNS'] = personal_pronoun_count\n","output['AVG WORD LENGTH'] = average_word_length\n","\n","output.to_csv('/content/drive/MyDrive/Data/Output Data Structure.csv', index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBX4NiXUC1jY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoroIxuBC1lu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_adTce3AC1oE"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO6U1pHQbLb/rlmAkF1KVPY","mount_file_id":"1ozsnsuIDzljYcbbu3BhoTDgkwylftib0","name":"Text_Analysis_Project.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
