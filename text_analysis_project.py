# -*- coding: utf-8 -*-
"""Text_Analysis_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ozsnsuIDzljYcbbu3BhoTDgkwylftib0

###  Importing all required libraries
---
"""

from bs4 import BeautifulSoup
import requests
import pandas as pd
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
nltk.download('punkt')
import numpy as np
from nltk.corpus import stopwords
nltk.download('stopwords')
import string
import re

"""**Taking all the required websites and generating the files of content from each website**"""

websites = pd.read_csv('/content/drive/MyDrive/Data/Websites.csv')
output = pd.read_csv('/content/drive/MyDrive/Data/Output.csv')

urls = websites['URL']  # all websites
content_list_of_all_websites = []  # list of content of all websites
personal_pronoun_count = []

headers = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64; rv:60.0) Gecko/20100101 Firefox/60.0"
}

def getContent(urls):    # will create a list of content from all the websites
    for url in urls :
        html_text = requests.get(url, headers=headers)
        soup = BeautifulSoup(html_text.content, 'html.parser')
        contents = soup.find_all('div', class_ = 'td-post-content')
        title = soup.find('header', class_ = 'td-post-title')
        title = title.find('h1').get_text()
        paragraph_clean = title
        personal_pronouns = ['I','we','my','ours','us']
        for data in contents:

            para = data.text
            for pronoun in personal_pronouns:
                pp_count = len(re.findall(pronoun, para))
            personal_pronoun_count.append(pp_count)
            paragraph_clean += para.replace('\xa0','') # removes unwanted text with ''

        try:
             paragraph_clean = paragraph_clean.replace(soup.find('pre').text, '')
        except :
            pass
        content_list_of_all_websites.append(paragraph_clean)

def createFiles():
    for i in range(len(content_list_of_all_websites)):
        file = open(str(i + 1) + '.txt', 'w')
        file.write(content_list_of_all_websites[i])
        file.close()



getContent(urls)  # initilaised the above function

createFiles()     # # creating text files named as respective url_id and saving the content in them

"""# creating stop words from given folder in drive and removing the stopwords from content
"""

allstopwords = []

def stopWords():
    files = ['/content/drive/MyDrive/Data/StopWords/StopWords_Auditor.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Currencies.txt','/content/drive/MyDrive/Data/StopWords/StopWords_DatesandNumbers.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Generic.txt','/content/drive/MyDrive/Data/StopWords/StopWords_GenericLong.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Geographic.txt','/content/drive/MyDrive/Data/StopWords/StopWords_Names.txt']
    stopwords = []
    for file in files:
        with open(file, 'rb') as f:
            a = f.read()
            stopwords.append(a.decode('latin-1').lower())
    for i in range(len(stopwords)):
        allstopwords.extend(stopwords[i].split())


stopWords()
allstopwords = set(allstopwords)

# tokenization of the content using the stop words created above and creating the list of remaining words(after removing stop words)

Content_withoutStopWords = []  # this contains the list of all the contents after removing the stop words
tokenized_content = []  # list of all the tokenized content

def remove_Stop_Words():
    for content in content_list_of_all_websites:
        tokenizedContent = word_tokenize(content.lower())
        tokenized_content.append(tokenizedContent)
        content_without_stop_words_current = []
        for word in tokenizedContent:
            if word not in allstopwords:
                content_without_stop_words_current.append(word)
        Content_withoutStopWords.append(content_without_stop_words_current)

remove_Stop_Words()

#Total Words after cleaning in each content

total_words_list_after_cleaning = []

for word_list in Content_withoutStopWords:
    total_words_list_after_cleaning.append(len(word_list))
total_words_list_after_cleaning = np.array(total_words_list_after_cleaning)

# print((Content_withoutStopWords))
# print(total_words_list_after_cleaning)

print((tokenized_content))

print((tokenized_content))

"""**Creating a dictionary of Positive and Negative words**

Calculating the following scores:


1.   Positive and Negative Score
2.   Polarity Score
3.   Subjectivity Score


"""

positive_words = []
negative_words = []

def pos_neg_words():
    files = ['/content/drive/MyDrive/Data/MasterDictionary/negative-words.txt','/content/drive/MyDrive/Data/MasterDictionary/positive-words.txt']

    for file in files:
        if file == '/content/drive/MyDrive/Data/MasterDictionary/negative-words.txt':
            with open(file, 'rb') as f:
                    a = f.read()
                    negative_words.extend((a.decode('latin-1').lower()).split())
        else:
            with open(file, 'rb') as f:
                    a = f.read()
                    positive_words.extend((a.decode('latin-1').lower()).split())

pos_neg_words()
positive_words = set(positive_words)
negative_words = set(negative_words)

# print(positive_words)
# print(negative_words)

# function to create dictionary of positive and negative words from the content collected from the websites

positive_words_list_from_content = []
negative_words_list_from_content = []

# function to create dictionary of positive and negative words from the content collected from the websites

positive_words_list_from_content = []
negative_words_list_from_content = []

def collect_Words_from_content():
    for words_of_each_content in Content_withoutStopWords:
        pos_words = []
        neg_words = []
        for word in words_of_each_content:
            if word in positive_words:
                pos_words.append(word)
            elif word in negative_words:
                neg_words.append(word)
        positive_words_list_from_content.append(set(pos_words))
        negative_words_list_from_content.append(set(neg_words))

collect_Words_from_content()
# print(positive_words_list_from_content)
# print(negative_words_list_from_content)

## Extracting Derived variables

# Positive And Negative score

PositiveScore = []
NegativeScore = []
def calculate_score():
    for p in positive_words_list_from_content:
        PositiveScore.append(len(p))
    for n in negative_words_list_from_content:
        NegativeScore.append(len(n))

calculate_score()
PositiveScore = np.array(PositiveScore)
NegativeScore = np.array(NegativeScore)
# print(PositiveScore)
# print(NegativeScore)

## Polarity score
Polarity_score = np.divide(np.subtract(PositiveScore,NegativeScore),(np.add(PositiveScore,NegativeScore)+0.000001))

# print(Polarity_score)

# Subjectivity Score

Subjectivity_score = np.divide(np.add(PositiveScore,NegativeScore),(total_words_list_after_cleaning+0.000001))

# print(Subjectivity_score)
# print(total_words_list_after_cleaning)

"""**Analysis of Readability (using nltk model stopwords) using the Gunning Fox index formula**

Calculating the following in each content:


1.   Total number words 
2.   Average Sentence Length
3.   Average Word Length
2.   Percentage Complex Words(>2 syllables)
1.   Syllables Per Word
2.   Personal Pronoun Count(calculated in 2 tab of this colab)




"""

PUNCT_TO_REMOVE = list(string.punctuation)
stpwrds = list(stopwords.words('english'))
PUNCT_TO_REMOVE.extend(stpwrds)
stop_words_from_nlkt = set(PUNCT_TO_REMOVE)

# print(PUNCT_TO_REMOVE)

Content_withoutStopWords_nltk = []  # this contains the list of all the contents after removing the stop words
total_character_count_after_cleaning = []
def remove_Stop_Words_nltk():
    for words in tokenized_content:
        content_without_stop_words_current = []
        count = 0
        for word in words:
            if word not in PUNCT_TO_REMOVE:
                content_without_stop_words_current.append(word)
                count += len(word)
        Content_withoutStopWords_nltk.append(content_without_stop_words_current)
        total_character_count_after_cleaning.append(count)
remove_Stop_Words_nltk()

# print(Content_withoutStopWords_nltk)

#Total Words after cleaning in each content using stopwords from nltk library

total_words_list_after_cleaning_nltk = []

for word_list in Content_withoutStopWords_nltk:
    total_words_list_after_cleaning_nltk.append(len(word_list))

total_words_list_after_cleaning_nltk = np.array(total_words_list_after_cleaning_nltk)
total_character_count_after_cleaning = np.array(total_character_count_after_cleaning)

# print(total_words_list_after_cleaning_nltk)


# Average Sentence Length

total_no_of_sentences = []
for lst in tokenized_content:
      total_no_of_sentences.append(lst.count('.')+lst.count('?')+lst.count('!'))

total_no_of_sentences = np.array(total_no_of_sentences)

average_senetence_length = np.divide(total_words_list_after_cleaning_nltk,total_no_of_sentences)

# Average Word Length

average_word_length = np.divide(total_character_count_after_cleaning,total_words_list_after_cleaning_nltk)

# print(total_no_of_sentences)
# print(average_word_length)

# Percentage of Complex words
no_of_complex_words = []

# coount syllable
syllable_count = []
def syllables(word):
    count = 0
    vowels = 'aeiou'
    exception_end_Tup = ('a','e','i','o','u','ed','es','le','les')

    if word[0] in vowels:
        count +=1
    for index in range(1,len(word)):
        if word[index] in vowels and word[index-1] not in vowels:
            count +=1
    if word.endswith(exception_end_Tup):
        count -= 1
    if count == 0:
        count += 1

    return count

for words in Content_withoutStopWords_nltk:
    i=0
    s_count = 0
    for word in words:
       syll_count = syllables(word)
       s_count += syll_count
       if syll_count >2:
           i+=1
    no_of_complex_words.append(i)
    syllable_count.append(s_count)

# print(Content_withoutStopWords_nltk)
# print(no_of_complex_words)

syllable_count = np.array(syllable_count)
# print(syllable_count)

no_of_complex_words = np.array(no_of_complex_words)
percentage_complex_words = np.divide(no_of_complex_words,total_words_list_after_cleaning_nltk)*100

#syllable count per word

syllable_count_per_word = np.divide(syllable_count,total_words_list_after_cleaning_nltk)

# print(percentage_complex_words)

# Fog Index

fog_index = 0.4*(np.add(percentage_complex_words,average_senetence_length))

# print(fog_index)

# Personal Pronouns

# print(personal_pronoun_count)

"""## Saving all the calculated terms to the CSV file"""

output = pd.read_csv('/content/drive/MyDrive/Data/Output.csv')
# print(output['URL_ID'])
# print(output.columns)

output['POSITIVE SCORE'] = PositiveScore
output['NEGATIVE SCORE'] = NegativeScore
output['POLARITY SCORE'] = Polarity_score
output['SUBJECTIVITY SCORE'] = Subjectivity_score
output['AVG SENTENCE LENGTH'] = average_senetence_length
output['PERCENTAGE OF COMPLEX WORDS'] = percentage_complex_words
output['FOG INDEX'] = fog_index
output['AVG NUMBER OF WORDS PER SENTENCE'] = average_senetence_length ##???
output['COMPLEX WORD COUNT'] = no_of_complex_words
output['WORD COUNT'] = total_words_list_after_cleaning_nltk
output['SYLLABLE PER WORD'] = syllable_count_per_word
output['PERSONAL PRONOUNS'] = personal_pronoun_count
output['AVG WORD LENGTH'] = average_word_length

output.to_csv('/content/drive/MyDrive/Data/Output Data Structure.csv', index=False)





